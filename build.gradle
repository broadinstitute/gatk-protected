buildscript {
    repositories {
        mavenCentral()
    }
}

plugins {
    id "java"  // set up default java compile and test tasks
    id "application"  // provides installDist
    id "jacoco"  // record code coverage during test execution
    id "com.github.johnrengelman.shadow" version "1.2.3"  //used to build the shadow and sparkJars
    id "com.github.kt3k.coveralls" version "2.6.3"  // report coverage data to coveralls
    id "com.github.ben-manes.versions" version "0.12.0" //used for identifying dependencies that need updating
}

mainClassName = "org.broadinstitute.hellbender.Main"


import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar
import org.gradle.internal.os.OperatingSystem

def isMacOsX = OperatingSystem.current().macOsX
def customJarPath = hasProperty("custom.jar.dir") ? property("custom.jar.dir") : null;

repositories {
    mavenCentral()
    maven {
      url "https://oss.sonatype.org/content/groups/public"
    }
    maven {
        url "https://repository.cloudera.com/artifactory/cloudera-repos/" // spark-dataflow
    }
    maven {
        url "https://artifactory.broadinstitute.org/artifactory/libs-snapshot/" //for htsjdk snapshots
    }

    // Locations for finding HDF and HDFJava jar files
    //  Please note that this is only for the jar files, not the jni shared native libraries (.so files --
    //   E.g. libjhdf5.so).  Instructions for those are found in README.md.
    //
    // If this falls out of date for Ubuntu:
    //  Locations for both the JNI and the jar files can be located at: http://packages.ubuntu.com/
    //
    //  Search for libjhdf5-java
    //  Select the desired Ubuntu build (e.g. vivid)
    //  Click "list of files" for the appropriate architecture
    //  Find the dir for the HDF jar files (listed in dependencies) and add it to the list below
    // If you would like the location of libjhdf5.so, you can search for "libjhdf5-jni" and look at the file listing.

    flatDir {
        // Ubuntu 12.04 through 15.04.  ``sudo apt-get install hdfview``  (aside:  shared libraries are in /usr/lib/jni/)
        // May not work in versions 15.10 and above
        dirs '/usr/share/java/'
    }

    flatDir {
        // Mac following dmg install instructions in README.
        dirs '/Applications/HDFView.app/Contents/Java/'
    }

    flatDir {
        // Travis
        dirs "$System.env.HDF5_DIR/HDFView/lib/"
    }

    flatDir {
        dirs '/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/'
    }

    if (customJarPath) {
        flatDir {
            // Specified by user
            dirs customJarPath
        }
    }

}

jacocoTestReport {
    dependsOn test
    group = "Reporting"
    description = "Generate Jacoco coverage reports after running tests."
    additionalSourceDirs = files(sourceSets.main.allJava.srcDirs)

    reports {
        xml.enabled = true // coveralls plugin depends on xml format report
        html.enabled = true
    }
}

jacoco {
    toolVersion = "0.7.1.201405082137"
}

//NOTE: we ignore contracts for now
compileJava {
   options.compilerArgs = ['-proc:none', '-Xlint:all', '-Werror']
}

compileTestJava {
   options.compilerArgs = ['-proc:none', '-Xlint:all', '-Werror']
}

build.dependsOn installDist
check.dependsOn installDist

dependencies {
    compile files("${System.properties['java.home']}/../lib/tools.jar")
    compile 'org.apache.commons:commons-math3:3.5'
    compile 'org.broadinstitute:gatk:4.alpha-122-ga4a6039-SNAPSHOT'
    compile 'com.opencsv:opencsv:3.4'
    compile 'org.ojalgo:ojalgo:39.0'
    compile ('org.apache.spark:spark-mllib_2.10:1.5.0') {
        // JUL is used by Google Dataflow as the backend logger, so exclude jul-to-slf4j to avoid a loop
        exclude module: 'jul-to-slf4j'
        exclude module: 'javax.servlet'
        exclude module: 'servlet-api'
    }

    // HDF5 and HDF-Java jar files
    if (isMacOsX) {
        compile name: 'jarhdf5-2.11.0'
    }
    else {
        compile name: 'jhdf5'
    }

    testCompile 'org.testng:testng:6.8.8'

    // Comment the next line to disable native code proxies in Spark MLLib
    compile('com.github.fommil.netlib:all:1.1.2')

    // Dependency change for including MLLib
    compile('org.objenesis:objenesis:1.2')
    testCompile('org.objenesis:objenesis:2.1')

    // Dependency change for including MLLib
    compile('de.javakaffee:kryo-serializers:0.26') {
        exclude module: 'kryo' // use Spark's version
    }

    // Dependency change for including MLLib
    compile('com.esotericsoftware:kryo:3.0.3'){
        exclude group: 'com.esotericsoftware', module: 'reflectasm'
        exclude group: 'org.ow2.asm', module: 'asm'
    }

    // Dependency change for including MLLib
    compile('com.esotericsoftware:reflectasm:1.10.0:shaded')
}

// Dependency change for including MLLib
configurations {
    compile.exclude module: 'jul-to-slf4j'
    compile.exclude module: 'javax.servlet'
    compile.exclude module: 'servlet-api'
    compile.exclude group: 'com.esotericsoftware.kryo'

    sparkConfiguration {
        extendsFrom runtime
        // exclude Hadoop and Spark dependencies, since they are provided when running with Spark
        // (ref: http://unethicalblogger.com/2015/07/15/gradle-goodness-excluding-depends-from-shadow.html)
        exclude group: 'org.apache.hadoop'
        exclude module: 'spark-core_2.10'
        exclude group: 'org.slf4j'
        exclude module: 'jul-to-slf4j'
        exclude module: 'javax.servlet'
        exclude module: 'servlet-api'
        exclude group: 'com.esotericsoftware.kryo'
        exclude module: 'spark-mllib_2.10'
        exclude group: 'org.scala-lang'
        exclude module: 'kryo'
    }
}

sourceCompatibility = 1.8
targetCompatibility = 1.8

def String deriveVersion(){
    def stdout = new ByteArrayOutputStream()
    try {
        logger.info("path is $System.env.PATH")
        exec {
            commandLine "git", "describe", "--always"
            standardOutput = stdout;

            ignoreExitValue = true
        }
    } catch (GradleException e) {
        logger.error("Couldn't determine version.  " + e.getMessage())
    }
    return stdout.size() > 0 ? stdout.toString().trim() : "version-unknown"
}

def createSymlinks(archivePath, symlinkLocation) {
    exec {
        commandLine 'ln', '-fs', archivePath, symlinkLocation
        ignoreExitValue = false
    }
}

// Suffix is what will be added to the symlink
def createAllSymlinks(destinationDir, archivePath, suffix) {
    def finalSuffix = "-" + suffix
    if (suffix == "") {
        finalSuffix = ""
    }

    def symlinkLocation = destinationDir.toString() + "/hellbender-protected" + finalSuffix + ".jar"
    def symlinkLocation2 = destinationDir.toString() + "/gatk-protected" + finalSuffix + ".jar"

    createSymlinks(archivePath.getAbsolutePath(), symlinkLocation)
    createSymlinks(archivePath.getAbsolutePath(), symlinkLocation2)
}

version = deriveVersion()
final SNAPSHOT = "-SNAPSHOT"
version = deriveVersion() + SNAPSHOT
boolean isRelease = ! version.endsWith(SNAPSHOT)
logger.info("build for version:" + version);
group = 'org.broadinstitute'


jar {
    manifest {
        attributes 'Implementation-Title': 'Hellbender-Protected-Tools',
                'Implementation-Version': version,
                'Main-Class': 'org.broadinstitute.hellbender.Main'
    }
}

// testJavaLibraryPath: Where to find jni libraries (e.g. HDF5's) to be used during testing.
// The environment variable JAVA_LIBRARY_PATH takes preference if present,
// Otherwise we used Gradle's 'testJavaLibraryPath' if present (use ~/.gradle.properties to addapt it to your dev. setup)
// or is left to the default otherwise. This default typically points to the usual system */lib directories.
def testJavaLibraryPath = System.env['JAVA_LIBRARY_PATH'] ?: hasProperty("testJavaLibraryPath") ? property("testJavaLibraryPath") : null;

test {
  
    if (testJavaLibraryPath) {
      systemProperties['java.library.path'] = testJavaLibraryPath
    }
    // enable TestNG support (default is JUnit)
    useTestNG{
        excludeGroups 'cloud', 'bucket'
    }

    // set heap size for the test JVM(s)
    jvmArgs = ['-Xmx6G']
  
    String CI = "$System.env.CI"

    maxParallelForks = CI == "true" ? 1 : 2

    if (CI == "true") {
        int count = 0
        // listen to events in the test execution lifecycle
        testLogging {
            events "skipped", "failed"
            exceptionFormat = "full"
        }

        beforeTest { descriptor ->
            count++
            if( count % 100 == 0) {
                logger.lifecycle("Finished "+ Integer.toString(count++) + " tests")
            }
        }
    } else {
        // show standard out and standard error of the test JVM(s) on the console
        testLogging.showStandardStreams = true
        beforeTest { descriptor ->
            logger.lifecycle("Running Test: " + descriptor)
        }

        // listen to standard out and standard error of the test JVM(s)
        onOutput { descriptor, event ->
            logger.lifecycle("Test: " + descriptor + " produced standard out/err: " + event.message )
        }
    }
}

task wrapper(type: Wrapper) {
    gradleVersion = '2.12'
}

tasks.withType(ShadowJar) {
    from(project.sourceSets.main.output)
    baseName = project.name + '-all'
    mergeServiceFiles()

    // Suggested by the akka devs to make sure that we do not get the spark configuration error.
    // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler
    transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {
        resource = 'reference.conf'
    }


    relocate 'com.google.common', 'org.broadinstitute.hellbender.relocated.com.google.common'
    zip64 true
    exclude 'log4j.properties' // from adam jar as it clashes with hellbender's log4j2.xml
}

shadowJar {
    classifier = 'spark_standalone'
    doLast {
        // Create a symlink to the newly created jar.  The name will be hellbender-protected.jar and
        //  it will be at the same level as the newly created jar.  (overwriting symlink, if it exists)
        // Please note that this will cause failures in Windows, which does not support symlinks.
        createAllSymlinks(destinationDir.toString(), archivePath, "")
    }
}

task sparkJar(type: ShadowJar) {
    configurations = [project.configurations.sparkConfiguration]
    classifier = 'spark'
    doLast {
        // Create a symlink to the newly created jar.  The name will be hellbender-protected.jar and
        //  it will be at the same level as the newly created jar.  (overwriting symlink, if it exists)
        // Please note that this will cause failures in Windows, which does not support symlinks.
        createAllSymlinks(destinationDir.toString(), archivePath, classifier)
    }
}

